<html lang="en"><head><meta charset="utf-8"/><title>Motor Representation</title><link href="https://fonts.googleapis.com/css?family=Lato:300,400,900,400italic" rel="stylesheet" type="text/css"/><link rel="stylesheet" href="/styles/for.slides.all.css"/><link rel="stylesheet" href="/styles/steve_deck_style.css"/></head><body><script defer="defer" src="/scripts/for.slides.all.min.js"></script><script>const wshost = (window.location.search.match(/[\?&]wshost=([\w\.]+)(&|$)/) || [])[1] 
const lead = (window.location.search.match(/[\?&]lead=(\w+)(&|$)/) || [])[1] || false
const pathname = window.location.pathname
console.log(`wshost: ${wshost}; lead: ${lead}`)
const doLeader = function (socket) {
  $(document).bind('deck.change', function (event, from, to) {
    const msg = JSON.stringify({from, to, pathname})
    socket.send(msg)
    console.log(`sent: ${msg}`)
  })
}
const doFollower = function (socket) {
  socket.addEventListener('message',function (msg) {
    const data = (() => {
      try {
        return JSON.parse(msg.data)
      } catch (e) {
        return msg.data
      }
    })()
    if( data && data.pathname && data.pathname === pathname ) {
      console.log(`remote control: going to slide ${data.to}`)
      $.deck('go', data.to)
    } else {
      console.log(msg)
    }
  })
}
const configure = function () {
  console.log(`configuring webscoket for remote control`)
  const socket = new WebSocket(`ws://${wshost}`)
  socket.onopen = function () {
    if (!lead || lead === 'false') {
      doFollower(socket)
    } else {
      doLeader(socket)
    }
  }
  return socket
}
if (wshost) {
  document.addEventListener("DOMContentLoaded", function(event) { 
    let socket = configure()
    window._socket = socket
    const monitorSocket = function () {
      if (socket.readyState !== 1) {
        console.log('websocket connection lost, attempting to reconnect')
        socket = configure()
      }
    }
    window.setInterval(monitorSocket, 1000)
  })
}
</script><div class="help-underlay" id="helpUnderlay"><div class="help-modal" id="helpModal"><h1>Keyboard Shortcuts<kbd class="help-key"><span>?f</span></kbd></h1><div class="help-close" id="helpClose">×</div><!-- .help-close--><div class="help-modal-content" id="helpModalContent"><div class="help-list-wrap" id="helpListWrap"><ul class="help-list"><li class="help-key-unit"><kbd class="help-key"><span>→</span></kbd><span class="help-key-def">Next step</span></li><li class="help-key-unit"><kbd class="help-key"><span>←</span></kbd><span class="help-key-def">Previous step</span></li><li class="help-key-unit"><kbd class="help-key"><span>↓</span></kbd><span class="help-key-def">Skip this slide</span></li><li class="help-key-unit"><kbd class="help-key"><span>↑</span></kbd><span class="help-key-def">Previous slide</span></li><li class="help-key-unit"><kbd class="help-key"><span>m</span></kbd><span class="help-key-def">Show slide thumbnails</span></li><li class="help-key-unit"><kbd class="help-key"><span>n</span></kbd><span class="help-key-def">Show notes</span></li><li class="help-key-unit"><kbd class="help-key"><span>h</span></kbd><span class="help-key-def">Show handout latex source</span></li><li class="help-key-unit"><kbd class="help-key"><span>N</span></kbd><span class="help-key-def">Show talk notes latex source</span></li></ul></div></div></div></div><div class="deck-notes"><div class="deck-notes-container"></div></div><div class="deck-handout"><div class="deck-handout-container"></div></div><div class="deck-container"><section class="slide" id="instructions"><div class="words"><div class="container_12"><div class="grid_12"><div class="middle"><p class="center">Click here and press the right key for the next slide.</p><p class="center">(This may not work on mobile or ipad. You can try using chrome or firefox, but even that may fail. Sorry.)</p></div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><p>also ...</p><p>Press the left key to go backwards (or swipe right)</p><p>Press n to toggle whether notes are shown (or add '?notes' to the url before the #)</p><p>Press m or double tap to slide thumbnails (menu)</p><p>Press ? at any time to show the keyboard shortcuts</p></div></div></div></section><script>window.deckSlideActions = window.deckSlideActions || [];
addAction = function(o) {
  // find which <script> element we are 
  //   (used by deck.init to work out which slide to attach the action to)
  var scripts = document.getElementsByTagName( 'script' );
  var thisScriptEl = scripts[ scripts.length - 1 ];
  o.scriptEl = thisScriptEl;
  window.deckSlideActions.push(o);
}

</script><!-- param @cls and param @options should be objects--><section class="slide" id="intro_to_motor_representation"><img class="bkg" src="/img/bkg/joint_action01/DSC_AB_6862.low.JPG"/><div class="spacer">&nbsp;</div><div class="title-block" style="position:relative; top:425px"><div class="title-container"><h2 class="title1">Motor Representation</h2></div></div></section><section class="slide"><img src="/img/merleau-ponty-smoking.jpg" style="transform:translateY(-210px)"/><div id="img-mask" style="position:absolute; top:0; left:0; right:0; bottom:0; min-width:1024px; min-height:768px; -webkit-mask: linear-gradient(transparent, black 70%); backdrop-filter: blur(20px); opacity: 1"></div><div id="img-mask2" style="position:absolute; top:0; left:0; right:0; bottom:0; min-width:1024px; min-height:768px; background-color: black; -webkit-mask: linear-gradient(transparent, black 80%)"></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="words"><div class="container_12"><div class="grid_12"><p style="margin-top: 300px;"><span>‘From the outset the grasping movement is </span><span class="magic">magically at its completion</span><span>; it can begin </span><span>only by anticipating its end, since to disallow taking hold is sufficient to inhibit the action.’ </span></p><p class="cite right"><span class='grey-text'>(Merleau-Ponty, 2012, p. 119)</span></p></div></div></div></div><div class="slide"><div class="dv dv-velocity" data-what="span:not(.magic), .cite" data-css="{&quot;opacity&quot;:0.5}" data-options="{&quot;duration&quot;:400}"></div></div></section><section class="slide"><img class="bkg" src="/img/egg.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p style="margin-top:4.5em;"><span class="some hide">some</span><br/><span>motor </span><span class="representation">representation</span><span class="step4 hide">s </span><span class="schema remove-me">(/schema) </span><span class="step4 hide">represent <span class="qqq">???</span></span><span class="step5 hide outcomes">outcomes<span class="goals remove-me"> = goals</span></span></p><div class="step3 hide"><div class="handout">‘a given motor act may change both as a function of what motor act will follow it—a sign of
planning—and as a function of what motor act preceded it—a sign
of memory’ <span class='grey-text'>(Cohen &#38; Rosenbaum, 2004, p. 294)</span>.</div><p>‘a given motor act may change ... as a function of what motor act will follow it—a sign of
planning’</p><p class="right grey-text">Cohen & Rosenbaum 2004, p. 294</p></div><div class="notes">Let me go back and start with some almost uncontroversial facts about motor representations and 
their action-coordinating role.</div><div class="notes">Why postulate motor representations at all?  [Dependence of present actions on future actions
is one reason for doing so.]</div><div class="noslide"><div class="notes">Suppose you are a cook who needs to take an egg from its box, crack it and put it (except for the
shell) into a bowl ready for beating into a carbonara sauce.
Even for such mundane, routine actions, the constraints on adequate performance can vary
significantly depending on subtle variations in context. For example, the position of a hot pan
may require altering the trajectory along which the egg is transported, or time pressures may mean
that the action must be performed unusually swiftly on this occasion.
Further, many of the constraints on performance involve relations between actions occurring at
different times.
To illustrate, how tightly you need to grip the egg now depends, among other things, on the forces
to which you will subject the egg in lifting it later.
It turns out that people reliably grip objects such as eggs just tightly enough across a range of
conditions in which the optimal tightness of grip varies.
This indicates (along with much other evidence) that information about the cook’s anticipated
future hand and arm movements appropriately influences how tightly she initially grips the egg
(compare <span class='grey-text'>Kawato, 1999</span>).
This anticipatory control of grasp, 
like several other features of action performance (<span class='grey-text'>Rosenbaum, 2010, p. chapter 1</span> for more examples),
is not plausibly a consequence of mindless physiology, nor of intention and practical reasoning.
This is one reason for postulating motor representations, which characteristically play a role in
coordinating sequences of very small scale actions such as grasping an egg in order to lift it.</div><div class="notes">[The scale of an actual action can be defined in terms of means-end relations. 
Given two actions which are related as means to ends by the processes and representations 
involved in their performance, the first is smaller in scale than the second just if the 
first is a means to the second.  Generalising, we associate the scale of an actual action 
with the depth of the hierarchy of outcomes that are related to it by the transitive closure 
of the means-ends relation.  Then, generalising further, a repeatable action (something that 
different agents might do independently on several occasions) is associated with a scale 
characteristic of the things people do when they perform that action.  Given that actions 
such as cooking a meal or painting a house count as small-scale actions, actions such as 
grasping an egg or dipping a brush into a can of paint are very-small scale.  Note that we 
do not stipulate a tight link between the very small scale and the motoric.  In some cases 
intentions may play a role in coordinating sequences of very small scale purposive actions, 
and in some cases motor representations may concern actions which are not very small scale. 
The claim we wish to consider is only that, often enough, explaining the coordination of 
sequences of very small scale actions appears to involve representations but not, or not 
only, intentions.  To a first approximation, \emph{motor representation} is a label for 
such representations.]</div></div><div class="slide"><div class="dv dv-addclass" data-what=".representation" data-cls="transition-04"></div><div class="dv dv-addclass" data-what=".representation" data-cls="bkg-orange"></div></div><div class="slide"><div class="dv dv-velocity" data-what=".step3" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><div class="notes">‘a given motor act may change both as a function of what motor act will follow it—a sign of
planning—and as a function of what motor act preceded it—a sign
of memory’ <span class='grey-text'>(Cohen &#38; Rosenbaum, 2004, p. 294)</span>.
</div></div><div class="slide"><div class="dv dv-removeclass" data-what=".representation" data-cls="bkg-orange"></div><div class="dv dv-velocity" data-what=".step3" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}"></div><div class="dv dv-velocity" data-what=".step4" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><div class="dv dv-velocity" data-what="img.bkg" data-css="{&quot;blur&quot;:&quot;50px&quot;}" data-options="{&quot;duration&quot;:1000}"></div><div class="notes">What do motor representations represent? An initially attractive, conservative 
view would be that they represent bodily configurations and joint displacements, 
or perhaps sequences of these, only.</div></div><div class="slide"><div class="dv dv-addclass" data-what=".qqq" data-cls="remove-me"></div><div class="dv dv-velocity" data-what=".step5" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><div class="notes">However there is now a significant body of evidence that some motor representations 
do not specify particular sequences of bodily configurations and joint displacements, 
but rather represent outcomes such as the grasping of an egg or the pressing of a switch. 
These are outcomes which might, on different occasions, involve very different bodily 
configurations and joint displacements
(see <span class='grey-text'>Rizzolatti &#38; Sinigaglia, 2010</span> for a selective review).</div><div class="notes">Such outcomes are abstract relative to bodily configurations and joint displacements 
in that there are many different ways of achieving them.


</div></div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/motor_representation_fig/Slide02.jpg" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">A goal represented motorically triggers a process which, via computations of things 
like end states, starting states and smoothness, eventually results in joint
displacements; and when things go well, these joint displacements together with the 
resulting bodily configurations bring about, or constitute, the occurrence of the goal.
</div></div></div></div></section><section class="slide"><img class="bkg" src="/img/motor_representation_fig/Slide03.jpg" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">But of course this is a simplification. Motor representations can trigger processes
which result in further goals being represented, as for example when a motor 
representation of the transporting of an object triggers representations of reaching,
grasping, placing and releasing.
</div></div></div></div></section><section class="slide"><img class="bkg" src="/img/motor_representation_fig/Slide04.jpg" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">The processes linking motor representations are planning like in two respects:
(i) means-end ...
</div></div></div></div></section><section class="slide"><img class="bkg" src="/img/motor_representation_fig/Slide05.jpg" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><div class="notes">... and (ii) relational constraints





</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">How do we know that outcomes are represented motorically?</p><div class="notes">But how do we know that motor representations carry information about such outcomes?
I’m glad you asked, let me explain ...
</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center huge-glow">background</p><p class="center"><span class="double">The Double Life of </span><span class="mr">Motor Representation</span></p></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><img src="/img/rizzolatti_2006_figure1_1.png"/></div></div></div></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="words"><div class="container_12"><div class="grid_12"><p class="source"><span class='grey-text'>(Rizzolatti &#38; Sinigaglia, 2008, p. figure 1.1)</span></p></div></div></div><div class="notes">‘The posterior section of the frontal lobe contains the motor areas,’ (p.~4)
Now you know as much about the brain as I do.</div><div class="notes">Mention primary and supplementary motor areas : we use the term ‘motor’ loosely
(compare ‘visual’, which also has narrower and broader uses in 
neuroscience).
</div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div class="fig1b"><div style="position:relative;"><img src="/img/fogassi_2005_fig1b.jpg" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div></div><div class="dv dv-style" data-what=".fig1b img" data-css="{&quot;filter&quot;:&quot;invert(1)&quot;}"></div><p class="source"><span class='grey-text'>Fogassi et al. (2005, p. figure 1b)</span></p><div class="notes">grasp to eat or grasp to place.</div><div class="notes">placing in the shoulder container minimizes kinematic differences
(This is not yet relevant; but will matter later for representing
outcomes vs kinematics as this could be used to illustrate how changing
the outcome changes the (marker of) representation)
</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div class="fogassi"><img src="/img/fogassi_2005_fig5.png" style="position:absolute;"/></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="fogassi2 hide"><img src="/img/fogassi_2005_fig5.png" style="position:absolute;"/></div></div><p class="source"><span class='grey-text'>Fogassi et al. (2005, p. figure 5)</span></p><div class="dv dv-style" data-what=".fogassi img" data-css="{&quot;clip&quot;:&quot;rect(0px,365px,200px,0)&quot;}"></div><div class="slide"><div class="dv dv-style" data-what=".fogassi img" data-css="{&quot;clip&quot;:&quot;rect(0px,365px,370px,0)&quot;}"></div></div><div class="slide"><div class="dv dv-velocity" data-what=".fogassi2" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><div class="dv dv-style" data-what=".fogassi2 img" data-css="{&quot;clip&quot;:&quot;rect(0px,999px,200px,0)&quot;}"></div></div><div class="slide"><div class="dv dv-style" data-what=".fogassi2 img" data-css="{&quot;clip&quot;:&quot;rect(0px,999px,370px,0)&quot;}"></div></div><div class="slide"><div class="notes">Same thing seen another way ...</div><div class="dv dv-style" data-what=".fogassi img" data-css="{&quot;clip&quot;:&quot;rect(0px,365px,999px,0)&quot;}"></div></div><div class="slide"><div class="dv dv-style" data-what=".fogassi2 img" data-css="{&quot;clip&quot;:&quot;rect(0px,999px,999px,0)&quot;}"></div></div><div class="notes">It is now a familiar, if still interestingly controversial idea, that motor representation
leads a DOUBLE LIFE.
For it is involved not only in coordinating the performance of small-scale purposive 
actions like reaching, grasping, placing and transporting but also in action observation.</div><div class="notes">If you were to observe someone phi-ing,
then motor representations would occur in you
much like those that would occur in you if it were you, not her, who was phi-ing.</div><div class="notes">This is not our focus,
it’s just a handy fact that simplifies testing.</div><div class="notes">We know this in large thanks to the discovery of mirror neurons and their consequences.</div><div class="notes">‘(A) Congruence between the visual and the motor response of a mirror neuron. Unit 169 has a stronger
discharge during grasping to eat than during grasping to place, both when the action is executed and
when it is observed. Conventions as in Fig. 1. (B) Population-averaged responses during motor and
visual tasks (12).’</div><div class="notes">Motor representations concerning a particular type of action are
involved not only in performing an action of that type but also sometimes in observing one. That is,
if you were to observe Ayesha reach for, grasp, transport and then place a pen, motor representations
would occur in you much like those that would also occur in you if it were you---not Ayesha---who was
doing this.</div><div class="notes">Converging evidence for this assertion comes from a variety of methods and measures;
but I won’t mention alll of that here. Except one TMS experiment ...
</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/dausilio_2009_fig1.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source"><span class='grey-text'>D’Ausilio et al. (2009, p. figure 1)</span></p><div class="notes">For a quick illustration of how we know about the double life of motor representation, consider
this experiment ....</div><div class="notes">‘Double TMS pulses were applied just prior to stimuli presentation to selectively prime the cortical
activity specifically in the lip (LipM1) or tongue (TongueM1) area’
<span class='grey-text'>(D’Ausilio et al., 2009, p. 381)</span></div><div class="notes">‘We hypothesized that focal stimulation would facilitate the perception of 
the concordant phonemes ([d] and [t] with TMS to TongueM1), but that 
there would be inhibition of perception of the discordant items 
([b] and [p] in this case). Behavioral effects were measured via reaction 
times (RTs) and error rates.’ <span class='grey-text'>(D’Ausilio et al., 2009, p. 382)</span>
</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><div style="position:relative;"><img src="/img/dausilio_2009_fig2.png" style="clip: auto; position: absolute; max-width:720px; max-height:550px;"/></div><p class="source"><span class='grey-text'>D’Ausilio et al. (2009, p. figure 2)</span></p><div class="notes">Prediction confirmed: TMS to lip area facilitates identification of labial 
phonetic gestures.</div><div class="notes">Labial = relating to the lips


</div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">[end of aside on the double life of motor representation]</div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><p class="center huge-glow-180" style="margin-top:-.3em">✔</p></div><p class="center huge-glow" style="mix-blend-mode: screen">background</p><p class="center"><span class="double">The Double Life of </span><span class="mr">Motor Representation</span></p></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">[end of aside on the double life of motor representation]</div><p class="center">How do we know that outcomes are represented motorically?</p><div class="notes"></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center">first illustration: same kinematics, different goal</p><div class="slide"><p class="em-above">glossary: TMS, MEP</p></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><img src="/img/villiger_2010_fig1ab.png"/><div class="step2 hide"><img src="/img/villiger_2010_fig1cd.png"/></div></div></div></div></div><div style="position: absolute; top:0; left:0; width:100%; height:100%;"><div class="words"><div class="container_12"><div class="grid_12"><p class="source"><span class='grey-text'>Villiger, Chandrasekharan, &#38; Welsh (2010, p. figure 1AB)</span></p></div></div></div></div><div class="notes">TMS to measure MEP</div><div class="notes">Read the frames left-to-right although the action direction is to the left (confusing)</div><div class="slide"><div class="dv dv-velocity" data-what=".step2" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><div class="notes">They also had an occluded end version ...
</div></div></section><section class="slide"><img class="bkg" src="/img/villiger_2010_fig2.png" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><div class="dv dv-style" data-what="img.bkg" data-css="{&quot;clip&quot;:&quot;rect(0px, 400px, 999px,0px)&quot;}"></div><p class="source"><span class='grey-text'>Villiger et al. (2010, p. figure 2)</span></p><div class="notes">Incidentally, ‘the observed direction of the modulation was not consistent with previous TMS
literature. Specifically, MEP amplitudes were significantly lower in the Object-Present than in the
Object-Absent conditions (Fig. 2), suggesting that there was an inhibitory effect of object
manipulation on the activity of M1 during action observation.’</div><div class="slide"><div class="dv dv-style" data-what="img.bkg" data-css="{&quot;clip&quot;:&quot;auto&quot;}"></div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p class="center step1">first illustration: same kinematics, different goal</p><div class="slide"><div class="dv dv-velocity" data-what=".step1" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:500}"></div><p class="center em-above">second illustration: different kinematics, same goal</p></div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/umilta_2008_figure1.png" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source"><span class='grey-text'>Umiltà et al. (2008, p. figure 1)</span></p><div class="notes">Umiltà et al, 2008 : single cell recordings in monkeys</div><div class="notes">MEPs (TMS amplified) in humans
</div></div></div></div></section><section class="slide"><img class="bkg" src="/img/cattaneo_2010_fig1.png" width="1024px"/><div class="words"><div class="container_12"><div class="grid_12"><p class="source"><span class='grey-text'>Cattaneo, Sandrini, &#38; Schwarzbach (2010, p. figure 1)</span></p><div class="notes">TMS MEP, humans.</div><div class="notes">Shown video, then a static picture.
Is this the same goal as you saw in the video?
Press one of two keys.
‘They were explicitly told to ignore the effector and make a judgment on the type of act only.’

</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><img src="/img/cattaneo_2010_fig3.png" style="filter:brightness(2) contrast(1.5)"/><p class="source"><span class='grey-text'>Cattaneo et al. (2010, p. figure 3)</span></p><div class="notes">Key finding: TMS to both ventral premotor cortex (PMv) and left supramarginal gyrus (SMG)
increases RTs regardless of whether it’s the same effector or a different effector.
(You can’t see same/different effector in this figure.)</div><div class="notes">KEY: superior temporal sulcus (STS), and a parietofrontal system consisting of the intraparietal
sulcus (IPS) and inferior parietal lobule (IPL) plus the ventral premotor cortex (PMv) and caudal
part of inferior frontal gyrus (IFG). In some instances also, the superior parietal lobule (SPL)

</div></div></div></div></section><section class="slide"><div class="words"><div class="container_12"><div class="grid_12"><img src="/img/cattaneo_2010_fig4.png" style="filter:brightness(2) contrast(1.5)"/><p class="source"><span class='grey-text'>Cattaneo et al. (2010, p. figure 4)</span></p><div class="notes">By contrast, TMS to superior temporal sulcus (STS) increased RT only for judgements
where the video effector was the same as the photo effector.
 
</div></div></div></div></section><section class="slide"><img class="bkg" src="/img/carry_information/Slide1.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="dv dv-velocity" data-what="img.bkg" data-css="{&quot;opacity&quot;:0}" data-options="{&quot;visibility&quot;:&quot;hidden&quot;,&quot;duration&quot;:0}"></div><div class="grid_6 words left-half"><div style="padding-right:1em;"><p>Markers of motor representation <span class="step2 hide"> ...</span></p><div class="notes">The experiments providing such evidence typically involve a marker of motor representation, 
such as a pattern of neuronal firings, a motor evoked potential or a behavioural performance 
profile, which, in controlled settings, allows sameness or difference of motor representation 
to be distinguished.  Such markers can be exploited to show that the sameness and difference 
of motor representation is linked to the sameness and difference of an outcome such as the 
grasping of a particular object.
(Pioneering uses of this method include <span class='grey-text'>G. Rizzolatti et al., 1988</span>; <span class='grey-text'>Giacomo Rizzolatti, Fogassi, &#38; Gallese, 2001</span>; 
it has since been developed in many ways: see, for example,
<span class='grey-text'>Hamilton &#38; Grafton (2008)</span>; <span class='grey-text'>Cattaneo, Caruana, Jezzini, &#38; Rizzolatti (2009)</span>; <span class='grey-text'>Cattaneo et al. (2010)</span>; <span class='grey-text'>Rochat et al. (2010)</span>; <span class='grey-text'>Bonini et al. (2010)</span>; <span class='grey-text'>Koch et al. (2010)</span>.)</div><div class="slide"><div class="dv dv-velocity" data-what="img.bkg" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><div class="dv dv-velocity" data-what=".step2" data-css="{&quot;opacity&quot;:1}" data-options="{&quot;visibility&quot;:&quot;visible&quot;,&quot;duration&quot;:400}"></div><p>1. are unaffected by variations in kinematic features but not goals</p></div><div class="slide"><div class="dv dv-attr" data-what="img.bkg" data-attr="src" data-value="/img/carry_information/Slide2.jpg"></div><p>2. are affected by variations in goals but not kinematic features</p></div><div class="slide"><div class="dv dv-attr" data-what="img.bkg" data-attr="src" data-value="/img/carry_information/Slide3.jpg"></div><div class="dv dv-velocity" data-what="img.bkg" data-css="{&quot;blur&quot;:&quot;5px&quot;}" data-options="{&quot;duration&quot;:500}"></div><p>So: 3. <span class="carry-information">carry information</span><span>  about goals  (from 1,2)</span></p></div><div class="slide"><div class="dv dv-addclass" data-what=".carry-information" data-cls="transition-04"></div><div class="dv dv-addclass" data-what=".carry-information" data-cls="bkg-invert"></div></div><div class="slide"><div class="dv dv-velocity" data-what="img.bkg" data-css="{&quot;blur&quot;:&quot;15px&quot;}" data-options="{&quot;duration&quot;:500}"></div><div class="dv dv-removeclass" data-what=".carry-information" data-cls="bkg-invert"></div><p class="em-above">Also</p><p>4. Information about outcomes guides planning-like processes ...</p></div></div></div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/planning_like/Slide1.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">To illustrate, consider a sequence of actions which might be involved in shoplifting an apple: you have to secure the apple, transport it, and position it in your pocket.
Each of these outcomes can be represented motorically.</div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/planning_like/Slide2.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">Motor processes are planning-like in that they involve computing means from ends.
Thus a representation of an end like securing it [the apple] can trigger a process 
that results in the representation of outcomes that are means to this end.
</div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/planning_like/Slide3.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">Motor processes are also planning-like in that which means are selected in preparing an 
action that will occur early in the sequence may affect needs that will arise only later 
in a later part of the actions.
For instance, how the apple is grasped at an early point in the sequence may be determined 
in part by what would be a more comfortable way for the other hand to grasp it later.</div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/planning_like/Slide4.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="notes">So motor representations of outcomes guide planning-like processes.
This is why I think it’s not just that they carry information about outcomes
like grasping an apple, but that they also represent such outcomes.



</div></div></div></div></div></section><section class="slide"><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><div class="grid_6 words left-half"><div style="padding-right:1em;"><p>Markers of motor representation <span class="step2 hide"> ...</span></p><p>1. are unaffected by variations in kinematic features but not goals</p><p>2. are affected by variations in goals but not kinematic features</p><p>So: </p><p>3. <span class="carry-information">carry information</span><span>  about goals  (from 1,2)</span></p><p class="em-above">Also</p><p>4. Information about outcomes guides planning-like processes.</p></div></div></div></div></div></div></section><section class="slide"><img class="bkg" src="/img/egg.jpg" width="1024px"/><div class="container_12"><div class="grid_12 "><div class="words"><div class="middle"><p style="margin-top:4.5em;"><span class="some">some</span><br/><span>motor representation<span class="step4">s </span><span class="schema remove-me">(/schema) </span><span class="step4">represent </span><span class="step5 outcomes">outcomes</span></span></p><div class="step3"><p>‘a given motor act may change ... as a function of what motor act will follow it—a sign of
planning’</p><p class="right grey-text">Cohen & Rosenbaum 2004, p. 294</p></div><div class="dv dv-velocity" data-what=".step3" data-css="{&quot;blur&quot;:&quot;2px&quot;}" data-options="{&quot;duration&quot;:0}"></div><div class="dv dv-velocity" data-what="img.bkg" data-css="{&quot;blur&quot;:&quot;50px&quot;}" data-options="{&quot;duration&quot;:0}"></div><div class="notes">In conclusion, two ideas about the control of action.
First, I follow <span class='grey-text'>Jeannerod (2006)</span> and others in  rejecting the view that all motor representations  specify only bodily configurations, joint displacements and end states.
Instead some motor representations specify outcomes to which actions are directed,
such as the grasping of a particular handle or the transporting of a given object.</div><div class="notes">Second, some motor processes involve computing means from ends and generating sensory expectations concerning the effects of actions <span class='grey-text'>(Wolpert, Doya, &#38; Kawato, 2003, p. e.g.)</span>.</div></div></div></div></div></section></div></body></html>